{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lesson 20 - Frequency Distributions.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"VuuiZYAMEOFR","colab_type":"text"},"cell_type":"markdown","source":["# 1.0 Frequency Distributions"]},{"metadata":{"id":"O4K6flK0t1Pk","colab_type":"code","outputId":"187470f7-9374-4dc7-fc6c-c90e6833a608","executionInfo":{"status":"ok","timestamp":1543336865561,"user_tz":180,"elapsed":16338,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":99}},"cell_type":"code","source":["# Uploading files from your local file system\n","\n","from google.colab import files\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-7d220b5b-2a8a-43b5-8035-79f3d888ca97\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-7d220b5b-2a8a-43b5-8035-79f3d888ca97\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving wnba.csv to wnba.csv\n","User uploaded file \"wnba.csv\" with length 20793 bytes\n"],"name":"stdout"}]},{"metadata":{"id":"spWyvb4nEtJc","colab_type":"text"},"cell_type":"markdown","source":["## 1.1 Simplifying Data"]},{"metadata":{"id":"CrzUD0iwEx6b","colab_type":"text"},"cell_type":"markdown","source":["Previously, we focused on the details around collecting data, on understanding its structure and how it's measured. **Collecting data is just the starting point in a data analysis workflow.** We rarely collect data just for the sake of collecting it. We collect data to analyze it, and we analyze it for different purposes:\n","\n","- To describe phenomena in the world (science).\n","- To make better decisions (industries).\n","- To improve systems (engineering).\n","- To describe different aspects of our society (data journalism); etc.\n","\n","<center><img width=\"250\" src=\"https://drive.google.com/uc?export=view&id=1_QX1g2KHfZVpg5mVsRUO4CJgp0F5StwP\"></center>\n","\n","\n","Our capacity to understand a data set just by looking at it in a table format is limited, and it decreases dramatically as the size of the data set increases. To be able to analyze data, we need to find ways to simplify it.\n","\n","The WNBA data set we've been working with has 143 rows and 32 columns. This might not seem like much compared to other data sets, but it's still extremely difficult to find any patterns just by eyeballing the data set in a table format. With 32 columns, even five rows would take us a couple of minutes to analyze:\n","\n","| _ | Name            | Team | Pos | Height | Weight | BMI       | Birth_Place | Birthdate         | Age | College        | Experience | Games Played | MIN |\n","|---|-----------------|------|-----|--------|--------|-----------|-------------|-------------------|-----|----------------|------------|--------------|-----|\n","| 0 | Aerial Powers   | DAL  | F   | 183    | 71.0   | 21.200991 | US          | January 17, 1994  | 23  | Michigan State | 2          | 8            | 173 |\n","| 1 | Alana Beard     | LA   | G/F | 185    | 73.0   | 21.329438 | US          | May 14, 1982      | 35  | Duke           | 12         | 30           | 947 |\n","| 2 | Alex Bentley    | CON  | G   | 170    | 69.0   | 23.875433 | US          | October 27, 1990  | 26  | Penn State     | 4          | 26           | 617 |\n","| 3 | Alex Montgomery | SAN  | G/F | 185    | 84.0   | 24.543462 | US          | December 11, 1988 | 28  | Georgia Tech   | 6          | 31           | 721 |\n","| 4 | Alexis Jones    | MIN  | G   | 175    | 78.0   | 25.469388 | US          | August 5, 1994    | 23  | Baylor         | R          | 24           | 137 |\n","\n","\n","One way to simplify this data set is to select a variable, count how many times each unique value occurs, and represent the frequencies (the number of times a unique value occurs) in a table. This is how such a table looks for the **POS (player position)** variable:\n","\n","<left><img width=\"150\" src=\"https://drive.google.com/uc?export=view&id=1la98ySTe-ElKFokRsXZTsUyb_RsuB8mL\"></left>\n","\n","\n","Because 60 of the players in our data set play as **guards**, the frequency for guards is 60. Because 33 of the players are forwards, the frequency for forwards is 33, and so on.\n","\n","With the table above, we simplified the **POS variable** by transforming it to a comprehensible format. Instead of having to deal with analyzing 143 values (the length of the POS variable), now we only have five values to analyze. We can make a few conclusions now that would have been difficult and time consuming to reach at just by looking at the list of 143 values:\n","\n","- We can see how the frequencies are distributed:\n","  - Almost half of the players play as guards.\n","  - Most of the players are either guards, forwards or centers.\n","  - Very few players have combined positions (like guard/forward or forward/center).\n","- We can make comparisons with ease:\n","  - There are roughly two times more guards than forwards.\n","  - There are slightly less centers that forwards; etc.\n","  \n","Because the table above shows how frequencies are distributed, it's often called a **frequency distribution table**, or, shorter, **frequency table** or **frequency distribution**. Throughout this mission, our focus will be on learning the details behind this form of simplifying data.\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","Try to get a sense for how difficult it is to analyze the **basketball data set** in its original form.\n","\n","- Read in the basketball data set (the name of the CSV file is **wnba.csv**) using **pd.read_csv()**.\n","- Using **DataFrame.shape**, find the number of rows and columns of the data set.\n","- Print the entire data set, and try to analyze the output to find some patterns."]},{"metadata":{"id":"zC9Pgny3FDFK","colab_type":"code","outputId":"c7fff021-4ad8-4c87-fe33-c1296a0dc2a5","executionInfo":{"status":"ok","timestamp":1543337015191,"user_tz":180,"elapsed":768,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":1992}},"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv(\"wnba.csv\")\n","data.shape\n","data\n","# put your code here"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Team</th>\n","      <th>Pos</th>\n","      <th>Height</th>\n","      <th>Weight</th>\n","      <th>BMI</th>\n","      <th>Birth_Place</th>\n","      <th>Birthdate</th>\n","      <th>Age</th>\n","      <th>College</th>\n","      <th>...</th>\n","      <th>OREB</th>\n","      <th>DREB</th>\n","      <th>REB</th>\n","      <th>AST</th>\n","      <th>STL</th>\n","      <th>BLK</th>\n","      <th>TO</th>\n","      <th>PTS</th>\n","      <th>DD2</th>\n","      <th>TD3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Aerial Powers</td>\n","      <td>DAL</td>\n","      <td>F</td>\n","      <td>183</td>\n","      <td>71.0</td>\n","      <td>21.200991</td>\n","      <td>US</td>\n","      <td>January 17, 1994</td>\n","      <td>23</td>\n","      <td>Michigan State</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>22</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>3</td>\n","      <td>6</td>\n","      <td>12</td>\n","      <td>93</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Alana Beard</td>\n","      <td>LA</td>\n","      <td>G/F</td>\n","      <td>185</td>\n","      <td>73.0</td>\n","      <td>21.329438</td>\n","      <td>US</td>\n","      <td>May 14, 1982</td>\n","      <td>35</td>\n","      <td>Duke</td>\n","      <td>...</td>\n","      <td>19</td>\n","      <td>82</td>\n","      <td>101</td>\n","      <td>72</td>\n","      <td>63</td>\n","      <td>13</td>\n","      <td>40</td>\n","      <td>217</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Alex Bentley</td>\n","      <td>CON</td>\n","      <td>G</td>\n","      <td>170</td>\n","      <td>69.0</td>\n","      <td>23.875433</td>\n","      <td>US</td>\n","      <td>October 27, 1990</td>\n","      <td>26</td>\n","      <td>Penn State</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>36</td>\n","      <td>40</td>\n","      <td>78</td>\n","      <td>22</td>\n","      <td>3</td>\n","      <td>24</td>\n","      <td>218</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Alex Montgomery</td>\n","      <td>SAN</td>\n","      <td>G/F</td>\n","      <td>185</td>\n","      <td>84.0</td>\n","      <td>24.543462</td>\n","      <td>US</td>\n","      <td>December 11, 1988</td>\n","      <td>28</td>\n","      <td>Georgia Tech</td>\n","      <td>...</td>\n","      <td>35</td>\n","      <td>134</td>\n","      <td>169</td>\n","      <td>65</td>\n","      <td>20</td>\n","      <td>10</td>\n","      <td>38</td>\n","      <td>188</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Alexis Jones</td>\n","      <td>MIN</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>78.0</td>\n","      <td>25.469388</td>\n","      <td>US</td>\n","      <td>August 5, 1994</td>\n","      <td>23</td>\n","      <td>Baylor</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>50</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Alexis Peterson</td>\n","      <td>SEA</td>\n","      <td>G</td>\n","      <td>170</td>\n","      <td>63.0</td>\n","      <td>21.799308</td>\n","      <td>US</td>\n","      <td>June 20, 1995</td>\n","      <td>22</td>\n","      <td>Syracuse</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>13</td>\n","      <td>16</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Alexis Prince</td>\n","      <td>PHO</td>\n","      <td>G</td>\n","      <td>188</td>\n","      <td>81.0</td>\n","      <td>22.917610</td>\n","      <td>US</td>\n","      <td>February 5, 1994</td>\n","      <td>23</td>\n","      <td>Baylor</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>15</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Allie Quigley</td>\n","      <td>CHI</td>\n","      <td>G</td>\n","      <td>178</td>\n","      <td>64.0</td>\n","      <td>20.199470</td>\n","      <td>US</td>\n","      <td>June 20, 1986</td>\n","      <td>31</td>\n","      <td>DePaul</td>\n","      <td>...</td>\n","      <td>9</td>\n","      <td>83</td>\n","      <td>92</td>\n","      <td>95</td>\n","      <td>20</td>\n","      <td>13</td>\n","      <td>59</td>\n","      <td>442</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Allisha Gray</td>\n","      <td>DAL</td>\n","      <td>G</td>\n","      <td>185</td>\n","      <td>76.0</td>\n","      <td>22.205990</td>\n","      <td>US</td>\n","      <td>October 20, 1992</td>\n","      <td>24</td>\n","      <td>South Carolina</td>\n","      <td>...</td>\n","      <td>52</td>\n","      <td>75</td>\n","      <td>127</td>\n","      <td>40</td>\n","      <td>47</td>\n","      <td>19</td>\n","      <td>37</td>\n","      <td>395</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Allison Hightower</td>\n","      <td>WAS</td>\n","      <td>G</td>\n","      <td>178</td>\n","      <td>77.0</td>\n","      <td>24.302487</td>\n","      <td>US</td>\n","      <td>June 4, 1988</td>\n","      <td>29</td>\n","      <td>LSU</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>36</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Alysha Clark</td>\n","      <td>SEA</td>\n","      <td>F</td>\n","      <td>180</td>\n","      <td>76.0</td>\n","      <td>23.456790</td>\n","      <td>US</td>\n","      <td>July 7, 1987</td>\n","      <td>30</td>\n","      <td>Middle Tennessee</td>\n","      <td>...</td>\n","      <td>29</td>\n","      <td>97</td>\n","      <td>126</td>\n","      <td>50</td>\n","      <td>22</td>\n","      <td>4</td>\n","      <td>32</td>\n","      <td>244</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Alyssa Thomas</td>\n","      <td>CON</td>\n","      <td>F</td>\n","      <td>188</td>\n","      <td>84.0</td>\n","      <td>23.766410</td>\n","      <td>US</td>\n","      <td>December 4, 1992</td>\n","      <td>24</td>\n","      <td>Maryland</td>\n","      <td>...</td>\n","      <td>34</td>\n","      <td>158</td>\n","      <td>192</td>\n","      <td>136</td>\n","      <td>48</td>\n","      <td>11</td>\n","      <td>87</td>\n","      <td>399</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Amanda Zahui B.</td>\n","      <td>NY</td>\n","      <td>C</td>\n","      <td>196</td>\n","      <td>113.0</td>\n","      <td>29.414827</td>\n","      <td>SE</td>\n","      <td>August 9, 1993</td>\n","      <td>24</td>\n","      <td>Minnesota</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>18</td>\n","      <td>23</td>\n","      <td>7</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>12</td>\n","      <td>51</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Amber Harris</td>\n","      <td>CHI</td>\n","      <td>F</td>\n","      <td>196</td>\n","      <td>88.0</td>\n","      <td>22.907122</td>\n","      <td>US</td>\n","      <td>January 16, 1988</td>\n","      <td>29</td>\n","      <td>Xavier</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>28</td>\n","      <td>40</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Aneika Henry</td>\n","      <td>ATL</td>\n","      <td>F/C</td>\n","      <td>193</td>\n","      <td>87.0</td>\n","      <td>23.356332</td>\n","      <td>JM</td>\n","      <td>February 13, 1986</td>\n","      <td>31</td>\n","      <td>Florida</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Angel Robinson</td>\n","      <td>PHO</td>\n","      <td>F/C</td>\n","      <td>198</td>\n","      <td>88.0</td>\n","      <td>22.446689</td>\n","      <td>US</td>\n","      <td>August 30, 1995</td>\n","      <td>21</td>\n","      <td>Arizona State</td>\n","      <td>...</td>\n","      <td>16</td>\n","      <td>42</td>\n","      <td>58</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>16</td>\n","      <td>58</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Asia Taylor</td>\n","      <td>WAS</td>\n","      <td>F</td>\n","      <td>185</td>\n","      <td>76.0</td>\n","      <td>22.205990</td>\n","      <td>US</td>\n","      <td>August 22, 1991</td>\n","      <td>26</td>\n","      <td>Louisville</td>\n","      <td>...</td>\n","      <td>16</td>\n","      <td>21</td>\n","      <td>37</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Bashaara Graves</td>\n","      <td>CHI</td>\n","      <td>F</td>\n","      <td>188</td>\n","      <td>91.0</td>\n","      <td>25.746944</td>\n","      <td>US</td>\n","      <td>March 17, 1994</td>\n","      <td>23</td>\n","      <td>Tennessee</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>13</td>\n","      <td>17</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Breanna Lewis</td>\n","      <td>DAL</td>\n","      <td>C</td>\n","      <td>196</td>\n","      <td>93.0</td>\n","      <td>24.208663</td>\n","      <td>US</td>\n","      <td>June 22, 1994</td>\n","      <td>23</td>\n","      <td>Kansas State</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Breanna Stewart</td>\n","      <td>SEA</td>\n","      <td>F/C</td>\n","      <td>193</td>\n","      <td>77.0</td>\n","      <td>20.671696</td>\n","      <td>US</td>\n","      <td>August 27, 1994</td>\n","      <td>22</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>43</td>\n","      <td>206</td>\n","      <td>249</td>\n","      <td>78</td>\n","      <td>29</td>\n","      <td>47</td>\n","      <td>68</td>\n","      <td>584</td>\n","      <td>8</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Bria Hartley</td>\n","      <td>NY</td>\n","      <td>G</td>\n","      <td>173</td>\n","      <td>66.0</td>\n","      <td>22.052190</td>\n","      <td>US</td>\n","      <td>September 30, 1992</td>\n","      <td>24</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>7</td>\n","      <td>50</td>\n","      <td>57</td>\n","      <td>58</td>\n","      <td>15</td>\n","      <td>5</td>\n","      <td>44</td>\n","      <td>217</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Bria Holmes</td>\n","      <td>ATL</td>\n","      <td>G</td>\n","      <td>185</td>\n","      <td>77.0</td>\n","      <td>22.498174</td>\n","      <td>US</td>\n","      <td>April 19, 1994</td>\n","      <td>23</td>\n","      <td>West Virginia</td>\n","      <td>...</td>\n","      <td>29</td>\n","      <td>56</td>\n","      <td>85</td>\n","      <td>52</td>\n","      <td>23</td>\n","      <td>7</td>\n","      <td>31</td>\n","      <td>235</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Briann January</td>\n","      <td>IND</td>\n","      <td>G</td>\n","      <td>173</td>\n","      <td>65.0</td>\n","      <td>21.718066</td>\n","      <td>US</td>\n","      <td>November 1, 1987</td>\n","      <td>29</td>\n","      <td>Arizona State</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>25</td>\n","      <td>37</td>\n","      <td>98</td>\n","      <td>23</td>\n","      <td>4</td>\n","      <td>53</td>\n","      <td>238</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Brionna Jones</td>\n","      <td>CON</td>\n","      <td>F</td>\n","      <td>191</td>\n","      <td>104.0</td>\n","      <td>28.507990</td>\n","      <td>US</td>\n","      <td>December 18, 1995</td>\n","      <td>21</td>\n","      <td>Maryland</td>\n","      <td>...</td>\n","      <td>11</td>\n","      <td>14</td>\n","      <td>25</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>44</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Brittany Boyd</td>\n","      <td>NY</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>71.0</td>\n","      <td>23.183673</td>\n","      <td>US</td>\n","      <td>November 6, 1993</td>\n","      <td>23</td>\n","      <td>UC Berkeley</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>8</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Brittney Griner</td>\n","      <td>PHO</td>\n","      <td>C</td>\n","      <td>206</td>\n","      <td>93.0</td>\n","      <td>21.915355</td>\n","      <td>US</td>\n","      <td>October 18, 1990</td>\n","      <td>26</td>\n","      <td>Baylor</td>\n","      <td>...</td>\n","      <td>43</td>\n","      <td>129</td>\n","      <td>172</td>\n","      <td>39</td>\n","      <td>13</td>\n","      <td>54</td>\n","      <td>52</td>\n","      <td>461</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Brittney Sykes</td>\n","      <td>ATL</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>66.0</td>\n","      <td>21.551020</td>\n","      <td>US</td>\n","      <td>July 2, 1994</td>\n","      <td>23</td>\n","      <td>Rutgers</td>\n","      <td>...</td>\n","      <td>25</td>\n","      <td>94</td>\n","      <td>119</td>\n","      <td>59</td>\n","      <td>18</td>\n","      <td>17</td>\n","      <td>49</td>\n","      <td>397</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Camille Little</td>\n","      <td>PHO</td>\n","      <td>F</td>\n","      <td>188</td>\n","      <td>82.0</td>\n","      <td>23.200543</td>\n","      <td>US</td>\n","      <td>January 18, 1985</td>\n","      <td>32</td>\n","      <td>North Carolina</td>\n","      <td>...</td>\n","      <td>42</td>\n","      <td>71</td>\n","      <td>113</td>\n","      <td>42</td>\n","      <td>28</td>\n","      <td>13</td>\n","      <td>50</td>\n","      <td>228</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Candace Parker</td>\n","      <td>LA</td>\n","      <td>F/C</td>\n","      <td>193</td>\n","      <td>79.0</td>\n","      <td>21.208623</td>\n","      <td>US</td>\n","      <td>April 19, 1986</td>\n","      <td>31</td>\n","      <td>Tennessee</td>\n","      <td>...</td>\n","      <td>37</td>\n","      <td>205</td>\n","      <td>242</td>\n","      <td>127</td>\n","      <td>43</td>\n","      <td>53</td>\n","      <td>80</td>\n","      <td>494</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Candice Dupree</td>\n","      <td>IND</td>\n","      <td>F</td>\n","      <td>188</td>\n","      <td>81.0</td>\n","      <td>22.917610</td>\n","      <td>US</td>\n","      <td>February 25, 1984</td>\n","      <td>33</td>\n","      <td>Temple</td>\n","      <td>...</td>\n","      <td>31</td>\n","      <td>124</td>\n","      <td>155</td>\n","      <td>47</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>42</td>\n","      <td>435</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>Sami Whitcomb</td>\n","      <td>SEA</td>\n","      <td>G</td>\n","      <td>178</td>\n","      <td>66.0</td>\n","      <td>20.830703</td>\n","      <td>US</td>\n","      <td>July 20, 1988</td>\n","      <td>29</td>\n","      <td>Washington</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>40</td>\n","      <td>52</td>\n","      <td>24</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>139</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>114</th>\n","      <td>Sancho Lyttle</td>\n","      <td>ATL</td>\n","      <td>F</td>\n","      <td>193</td>\n","      <td>79.0</td>\n","      <td>21.208623</td>\n","      <td>ES</td>\n","      <td>September 20, 1983</td>\n","      <td>33</td>\n","      <td>Houston</td>\n","      <td>...</td>\n","      <td>42</td>\n","      <td>138</td>\n","      <td>180</td>\n","      <td>41</td>\n","      <td>40</td>\n","      <td>17</td>\n","      <td>34</td>\n","      <td>156</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>115</th>\n","      <td>Sandrine Gruda</td>\n","      <td>LA</td>\n","      <td>F/C</td>\n","      <td>193</td>\n","      <td>84.0</td>\n","      <td>22.550941</td>\n","      <td>FR</td>\n","      <td>June 25, 1987</td>\n","      <td>30</td>\n","      <td>France</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>116</th>\n","      <td>Saniya Chong</td>\n","      <td>DAL</td>\n","      <td>G</td>\n","      <td>173</td>\n","      <td>64.0</td>\n","      <td>21.383942</td>\n","      <td>US</td>\n","      <td>June 27, 1994</td>\n","      <td>23</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>9</td>\n","      <td>19</td>\n","      <td>28</td>\n","      <td>33</td>\n","      <td>21</td>\n","      <td>3</td>\n","      <td>23</td>\n","      <td>87</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>117</th>\n","      <td>Seimone Augustus</td>\n","      <td>MIN</td>\n","      <td>G/F</td>\n","      <td>183</td>\n","      <td>77.0</td>\n","      <td>22.992624</td>\n","      <td>US</td>\n","      <td>April 30, 1984</td>\n","      <td>33</td>\n","      <td>LSU</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>70</td>\n","      <td>82</td>\n","      <td>108</td>\n","      <td>17</td>\n","      <td>1</td>\n","      <td>39</td>\n","      <td>298</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>118</th>\n","      <td>Sequoia Holmes</td>\n","      <td>SAN</td>\n","      <td>G</td>\n","      <td>185</td>\n","      <td>70.0</td>\n","      <td>20.452885</td>\n","      <td>US</td>\n","      <td>June 13, 1986</td>\n","      <td>31</td>\n","      <td>UNLV</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>24</td>\n","      <td>23</td>\n","      <td>13</td>\n","      <td>5</td>\n","      <td>11</td>\n","      <td>81</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>Shatori Walker-Kimbrough</td>\n","      <td>WAS</td>\n","      <td>G</td>\n","      <td>180</td>\n","      <td>64.0</td>\n","      <td>19.753086</td>\n","      <td>US</td>\n","      <td>May 18, 1995</td>\n","      <td>22</td>\n","      <td>Maryland</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>13</td>\n","      <td>17</td>\n","      <td>10</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>96</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>120</th>\n","      <td>Shavonte Zellous</td>\n","      <td>NY</td>\n","      <td>G</td>\n","      <td>178</td>\n","      <td>85.0</td>\n","      <td>26.827421</td>\n","      <td>US</td>\n","      <td>August 28, 1986</td>\n","      <td>30</td>\n","      <td>Pittsburgh</td>\n","      <td>...</td>\n","      <td>30</td>\n","      <td>92</td>\n","      <td>122</td>\n","      <td>87</td>\n","      <td>23</td>\n","      <td>8</td>\n","      <td>62</td>\n","      <td>346</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>121</th>\n","      <td>Shay Murphy</td>\n","      <td>SAN</td>\n","      <td>G</td>\n","      <td>180</td>\n","      <td>74.0</td>\n","      <td>22.839506</td>\n","      <td>US</td>\n","      <td>April 15, 1985</td>\n","      <td>32</td>\n","      <td>Southern California</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>26</td>\n","      <td>38</td>\n","      <td>17</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>66</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>122</th>\n","      <td>Shekinna Stricklen</td>\n","      <td>CON</td>\n","      <td>G/F</td>\n","      <td>188</td>\n","      <td>81.0</td>\n","      <td>22.917610</td>\n","      <td>US</td>\n","      <td>July 30, 1990</td>\n","      <td>27</td>\n","      <td>Tennessee</td>\n","      <td>...</td>\n","      <td>15</td>\n","      <td>71</td>\n","      <td>86</td>\n","      <td>30</td>\n","      <td>36</td>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>245</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>123</th>\n","      <td>Shenise Johnson</td>\n","      <td>IND</td>\n","      <td>G</td>\n","      <td>180</td>\n","      <td>78.0</td>\n","      <td>24.074074</td>\n","      <td>US</td>\n","      <td>September 12, 1990</td>\n","      <td>26</td>\n","      <td>Miami (FL)</td>\n","      <td>...</td>\n","      <td>13</td>\n","      <td>35</td>\n","      <td>48</td>\n","      <td>35</td>\n","      <td>21</td>\n","      <td>4</td>\n","      <td>18</td>\n","      <td>158</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>124</th>\n","      <td>Skylar Diggins-Smith</td>\n","      <td>DAL</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>66.0</td>\n","      <td>21.551020</td>\n","      <td>US</td>\n","      <td>February 8, 1990</td>\n","      <td>27</td>\n","      <td>Notre Dame</td>\n","      <td>...</td>\n","      <td>21</td>\n","      <td>86</td>\n","      <td>107</td>\n","      <td>173</td>\n","      <td>38</td>\n","      <td>24</td>\n","      <td>83</td>\n","      <td>545</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>125</th>\n","      <td>Stefanie Dolson</td>\n","      <td>CHI</td>\n","      <td>C</td>\n","      <td>196</td>\n","      <td>97.0</td>\n","      <td>25.249896</td>\n","      <td>US</td>\n","      <td>August 1, 1992</td>\n","      <td>25</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>35</td>\n","      <td>121</td>\n","      <td>156</td>\n","      <td>65</td>\n","      <td>14</td>\n","      <td>37</td>\n","      <td>65</td>\n","      <td>398</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>126</th>\n","      <td>Stephanie Talbot</td>\n","      <td>PHO</td>\n","      <td>G</td>\n","      <td>185</td>\n","      <td>87.0</td>\n","      <td>25.420015</td>\n","      <td>AU</td>\n","      <td>December 20, 1990</td>\n","      <td>26</td>\n","      <td>Australia</td>\n","      <td>...</td>\n","      <td>28</td>\n","      <td>58</td>\n","      <td>86</td>\n","      <td>50</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>138</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>127</th>\n","      <td>Sue Bird</td>\n","      <td>SEA</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>68.0</td>\n","      <td>22.204082</td>\n","      <td>US</td>\n","      <td>October 16, 1980</td>\n","      <td>36</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>7</td>\n","      <td>46</td>\n","      <td>53</td>\n","      <td>177</td>\n","      <td>31</td>\n","      <td>3</td>\n","      <td>57</td>\n","      <td>273</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>128</th>\n","      <td>Sugar Rodgers</td>\n","      <td>NY</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>75.0</td>\n","      <td>24.489796</td>\n","      <td>US</td>\n","      <td>August 12, 1989</td>\n","      <td>28</td>\n","      <td>Georgetown</td>\n","      <td>...</td>\n","      <td>21</td>\n","      <td>85</td>\n","      <td>106</td>\n","      <td>68</td>\n","      <td>28</td>\n","      <td>17</td>\n","      <td>43</td>\n","      <td>317</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>129</th>\n","      <td>Sydney Colson</td>\n","      <td>SAN</td>\n","      <td>G</td>\n","      <td>173</td>\n","      <td>64.0</td>\n","      <td>21.383942</td>\n","      <td>US</td>\n","      <td>June 8, 1989</td>\n","      <td>28</td>\n","      <td>Texas A&amp;M</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>14</td>\n","      <td>51</td>\n","      <td>13</td>\n","      <td>2</td>\n","      <td>25</td>\n","      <td>72</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>130</th>\n","      <td>Sydney Wiese</td>\n","      <td>LA</td>\n","      <td>G</td>\n","      <td>183</td>\n","      <td>68.0</td>\n","      <td>20.305175</td>\n","      <td>US</td>\n","      <td>July 13, 1992</td>\n","      <td>25</td>\n","      <td>Oregon State</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>18</td>\n","      <td>21</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>55</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>131</th>\n","      <td>Sylvia Fowles</td>\n","      <td>MIN</td>\n","      <td>C</td>\n","      <td>198</td>\n","      <td>96.0</td>\n","      <td>24.487297</td>\n","      <td>US</td>\n","      <td>June 10, 1985</td>\n","      <td>32</td>\n","      <td>LSU</td>\n","      <td>...</td>\n","      <td>113</td>\n","      <td>184</td>\n","      <td>297</td>\n","      <td>39</td>\n","      <td>39</td>\n","      <td>61</td>\n","      <td>71</td>\n","      <td>572</td>\n","      <td>16</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>Tamera Young</td>\n","      <td>ATL</td>\n","      <td>G/F</td>\n","      <td>188</td>\n","      <td>77.0</td>\n","      <td>21.785876</td>\n","      <td>US</td>\n","      <td>October 30, 1986</td>\n","      <td>30</td>\n","      <td>Tennessee</td>\n","      <td>...</td>\n","      <td>23</td>\n","      <td>87</td>\n","      <td>110</td>\n","      <td>66</td>\n","      <td>36</td>\n","      <td>14</td>\n","      <td>61</td>\n","      <td>277</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>133</th>\n","      <td>Tayler Hill</td>\n","      <td>WAS</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>66.0</td>\n","      <td>21.551020</td>\n","      <td>US</td>\n","      <td>October 23, 1990</td>\n","      <td>26</td>\n","      <td>Ohio State</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>29</td>\n","      <td>34</td>\n","      <td>47</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>240</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>134</th>\n","      <td>Temi Fagbenle</td>\n","      <td>MIN</td>\n","      <td>C</td>\n","      <td>193</td>\n","      <td>89.0</td>\n","      <td>23.893259</td>\n","      <td>UK</td>\n","      <td>August 9, 1992</td>\n","      <td>25</td>\n","      <td>Southern California</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>13</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>135</th>\n","      <td>Theresa Plaisance</td>\n","      <td>DAL</td>\n","      <td>F</td>\n","      <td>196</td>\n","      <td>91.0</td>\n","      <td>23.688047</td>\n","      <td>US</td>\n","      <td>May 18, 1992</td>\n","      <td>25</td>\n","      <td>LSU</td>\n","      <td>...</td>\n","      <td>38</td>\n","      <td>89</td>\n","      <td>127</td>\n","      <td>24</td>\n","      <td>23</td>\n","      <td>22</td>\n","      <td>24</td>\n","      <td>217</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>136</th>\n","      <td>Tianna Hawkins</td>\n","      <td>WAS</td>\n","      <td>F</td>\n","      <td>191</td>\n","      <td>87.0</td>\n","      <td>23.848030</td>\n","      <td>US</td>\n","      <td>February 3, 1991</td>\n","      <td>26</td>\n","      <td>Maryland</td>\n","      <td>...</td>\n","      <td>42</td>\n","      <td>82</td>\n","      <td>124</td>\n","      <td>9</td>\n","      <td>15</td>\n","      <td>7</td>\n","      <td>23</td>\n","      <td>210</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>137</th>\n","      <td>Tierra Ruffin-Pratt</td>\n","      <td>WAS</td>\n","      <td>G</td>\n","      <td>178</td>\n","      <td>83.0</td>\n","      <td>26.196187</td>\n","      <td>US</td>\n","      <td>November 4, 1991</td>\n","      <td>25</td>\n","      <td>North Carolina</td>\n","      <td>...</td>\n","      <td>45</td>\n","      <td>120</td>\n","      <td>165</td>\n","      <td>68</td>\n","      <td>30</td>\n","      <td>16</td>\n","      <td>47</td>\n","      <td>225</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>138</th>\n","      <td>Tiffany Hayes</td>\n","      <td>ATL</td>\n","      <td>G</td>\n","      <td>178</td>\n","      <td>70.0</td>\n","      <td>22.093170</td>\n","      <td>US</td>\n","      <td>September 20, 1989</td>\n","      <td>27</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>28</td>\n","      <td>89</td>\n","      <td>117</td>\n","      <td>69</td>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>50</td>\n","      <td>467</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>139</th>\n","      <td>Tiffany Jackson</td>\n","      <td>LA</td>\n","      <td>F</td>\n","      <td>191</td>\n","      <td>84.0</td>\n","      <td>23.025685</td>\n","      <td>US</td>\n","      <td>April 26, 1985</td>\n","      <td>32</td>\n","      <td>Texas</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>18</td>\n","      <td>23</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>140</th>\n","      <td>Tiffany Mitchell</td>\n","      <td>IND</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>69.0</td>\n","      <td>22.530612</td>\n","      <td>US</td>\n","      <td>September 23, 1984</td>\n","      <td>32</td>\n","      <td>South Carolina</td>\n","      <td>...</td>\n","      <td>16</td>\n","      <td>70</td>\n","      <td>86</td>\n","      <td>39</td>\n","      <td>31</td>\n","      <td>5</td>\n","      <td>40</td>\n","      <td>277</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>Tina Charles</td>\n","      <td>NY</td>\n","      <td>F/C</td>\n","      <td>193</td>\n","      <td>84.0</td>\n","      <td>22.550941</td>\n","      <td>US</td>\n","      <td>May 12, 1988</td>\n","      <td>29</td>\n","      <td>Connecticut</td>\n","      <td>...</td>\n","      <td>56</td>\n","      <td>212</td>\n","      <td>268</td>\n","      <td>75</td>\n","      <td>21</td>\n","      <td>22</td>\n","      <td>71</td>\n","      <td>582</td>\n","      <td>11</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>Yvonne Turner</td>\n","      <td>PHO</td>\n","      <td>G</td>\n","      <td>175</td>\n","      <td>59.0</td>\n","      <td>19.265306</td>\n","      <td>US</td>\n","      <td>October 13, 1987</td>\n","      <td>29</td>\n","      <td>Nebraska</td>\n","      <td>...</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>24</td>\n","      <td>30</td>\n","      <td>18</td>\n","      <td>1</td>\n","      <td>32</td>\n","      <td>151</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>143 rows × 32 columns</p>\n","</div>"],"text/plain":["                         Name Team  Pos  Height  Weight        BMI  \\\n","0               Aerial Powers  DAL    F     183    71.0  21.200991   \n","1                 Alana Beard   LA  G/F     185    73.0  21.329438   \n","2                Alex Bentley  CON    G     170    69.0  23.875433   \n","3             Alex Montgomery  SAN  G/F     185    84.0  24.543462   \n","4                Alexis Jones  MIN    G     175    78.0  25.469388   \n","5             Alexis Peterson  SEA    G     170    63.0  21.799308   \n","6               Alexis Prince  PHO    G     188    81.0  22.917610   \n","7               Allie Quigley  CHI    G     178    64.0  20.199470   \n","8                Allisha Gray  DAL    G     185    76.0  22.205990   \n","9           Allison Hightower  WAS    G     178    77.0  24.302487   \n","10               Alysha Clark  SEA    F     180    76.0  23.456790   \n","11              Alyssa Thomas  CON    F     188    84.0  23.766410   \n","12            Amanda Zahui B.   NY    C     196   113.0  29.414827   \n","13               Amber Harris  CHI    F     196    88.0  22.907122   \n","14               Aneika Henry  ATL  F/C     193    87.0  23.356332   \n","15             Angel Robinson  PHO  F/C     198    88.0  22.446689   \n","16                Asia Taylor  WAS    F     185    76.0  22.205990   \n","17            Bashaara Graves  CHI    F     188    91.0  25.746944   \n","18              Breanna Lewis  DAL    C     196    93.0  24.208663   \n","19            Breanna Stewart  SEA  F/C     193    77.0  20.671696   \n","20               Bria Hartley   NY    G     173    66.0  22.052190   \n","21                Bria Holmes  ATL    G     185    77.0  22.498174   \n","22             Briann January  IND    G     173    65.0  21.718066   \n","23              Brionna Jones  CON    F     191   104.0  28.507990   \n","24              Brittany Boyd   NY    G     175    71.0  23.183673   \n","25            Brittney Griner  PHO    C     206    93.0  21.915355   \n","26             Brittney Sykes  ATL    G     175    66.0  21.551020   \n","27             Camille Little  PHO    F     188    82.0  23.200543   \n","28             Candace Parker   LA  F/C     193    79.0  21.208623   \n","29             Candice Dupree  IND    F     188    81.0  22.917610   \n","..                        ...  ...  ...     ...     ...        ...   \n","113             Sami Whitcomb  SEA    G     178    66.0  20.830703   \n","114             Sancho Lyttle  ATL    F     193    79.0  21.208623   \n","115            Sandrine Gruda   LA  F/C     193    84.0  22.550941   \n","116              Saniya Chong  DAL    G     173    64.0  21.383942   \n","117          Seimone Augustus  MIN  G/F     183    77.0  22.992624   \n","118            Sequoia Holmes  SAN    G     185    70.0  20.452885   \n","119  Shatori Walker-Kimbrough  WAS    G     180    64.0  19.753086   \n","120          Shavonte Zellous   NY    G     178    85.0  26.827421   \n","121               Shay Murphy  SAN    G     180    74.0  22.839506   \n","122        Shekinna Stricklen  CON  G/F     188    81.0  22.917610   \n","123           Shenise Johnson  IND    G     180    78.0  24.074074   \n","124      Skylar Diggins-Smith  DAL    G     175    66.0  21.551020   \n","125           Stefanie Dolson  CHI    C     196    97.0  25.249896   \n","126          Stephanie Talbot  PHO    G     185    87.0  25.420015   \n","127                  Sue Bird  SEA    G     175    68.0  22.204082   \n","128             Sugar Rodgers   NY    G     175    75.0  24.489796   \n","129             Sydney Colson  SAN    G     173    64.0  21.383942   \n","130              Sydney Wiese   LA    G     183    68.0  20.305175   \n","131             Sylvia Fowles  MIN    C     198    96.0  24.487297   \n","132              Tamera Young  ATL  G/F     188    77.0  21.785876   \n","133               Tayler Hill  WAS    G     175    66.0  21.551020   \n","134             Temi Fagbenle  MIN    C     193    89.0  23.893259   \n","135         Theresa Plaisance  DAL    F     196    91.0  23.688047   \n","136            Tianna Hawkins  WAS    F     191    87.0  23.848030   \n","137       Tierra Ruffin-Pratt  WAS    G     178    83.0  26.196187   \n","138             Tiffany Hayes  ATL    G     178    70.0  22.093170   \n","139           Tiffany Jackson   LA    F     191    84.0  23.025685   \n","140          Tiffany Mitchell  IND    G     175    69.0  22.530612   \n","141              Tina Charles   NY  F/C     193    84.0  22.550941   \n","142             Yvonne Turner  PHO    G     175    59.0  19.265306   \n","\n","    Birth_Place           Birthdate  Age              College ...  OREB  DREB  \\\n","0            US    January 17, 1994   23       Michigan State ...     6    22   \n","1            US        May 14, 1982   35                 Duke ...    19    82   \n","2            US    October 27, 1990   26           Penn State ...     4    36   \n","3            US   December 11, 1988   28         Georgia Tech ...    35   134   \n","4            US      August 5, 1994   23               Baylor ...     3     9   \n","5            US       June 20, 1995   22             Syracuse ...     3    13   \n","6            US    February 5, 1994   23               Baylor ...     1    14   \n","7            US       June 20, 1986   31               DePaul ...     9    83   \n","8            US    October 20, 1992   24       South Carolina ...    52    75   \n","9            US        June 4, 1988   29                  LSU ...     3     7   \n","10           US        July 7, 1987   30     Middle Tennessee ...    29    97   \n","11           US    December 4, 1992   24             Maryland ...    34   158   \n","12           SE      August 9, 1993   24            Minnesota ...     5    18   \n","13           US    January 16, 1988   29               Xavier ...    12    28   \n","14           JM   February 13, 1986   31              Florida ...     0     4   \n","15           US     August 30, 1995   21        Arizona State ...    16    42   \n","16           US     August 22, 1991   26           Louisville ...    16    21   \n","17           US      March 17, 1994   23            Tennessee ...     4    13   \n","18           US       June 22, 1994   23         Kansas State ...     2     7   \n","19           US     August 27, 1994   22          Connecticut ...    43   206   \n","20           US  September 30, 1992   24          Connecticut ...     7    50   \n","21           US      April 19, 1994   23        West Virginia ...    29    56   \n","22           US    November 1, 1987   29        Arizona State ...    12    25   \n","23           US   December 18, 1995   21             Maryland ...    11    14   \n","24           US    November 6, 1993   23          UC Berkeley ...     3     5   \n","25           US    October 18, 1990   26               Baylor ...    43   129   \n","26           US        July 2, 1994   23              Rutgers ...    25    94   \n","27           US    January 18, 1985   32       North Carolina ...    42    71   \n","28           US      April 19, 1986   31            Tennessee ...    37   205   \n","29           US   February 25, 1984   33               Temple ...    31   124   \n","..          ...                 ...  ...                  ... ...   ...   ...   \n","113          US       July 20, 1988   29           Washington ...    12    40   \n","114          ES  September 20, 1983   33              Houston ...    42   138   \n","115          FR       June 25, 1987   30               France ...     0     2   \n","116          US       June 27, 1994   23          Connecticut ...     9    19   \n","117          US      April 30, 1984   33                  LSU ...    12    70   \n","118          US       June 13, 1986   31                 UNLV ...    12    12   \n","119          US        May 18, 1995   22             Maryland ...     4    13   \n","120          US     August 28, 1986   30           Pittsburgh ...    30    92   \n","121          US      April 15, 1985   32  Southern California ...    12    26   \n","122          US       July 30, 1990   27            Tennessee ...    15    71   \n","123          US  September 12, 1990   26           Miami (FL) ...    13    35   \n","124          US    February 8, 1990   27           Notre Dame ...    21    86   \n","125          US      August 1, 1992   25          Connecticut ...    35   121   \n","126          AU   December 20, 1990   26            Australia ...    28    58   \n","127          US    October 16, 1980   36          Connecticut ...     7    46   \n","128          US     August 12, 1989   28           Georgetown ...    21    85   \n","129          US        June 8, 1989   28            Texas A&M ...     3    11   \n","130          US       July 13, 1992   25         Oregon State ...     3    18   \n","131          US       June 10, 1985   32                  LSU ...   113   184   \n","132          US    October 30, 1986   30            Tennessee ...    23    87   \n","133          US    October 23, 1990   26           Ohio State ...     5    29   \n","134          UK      August 9, 1992   25  Southern California ...     3    13   \n","135          US        May 18, 1992   25                  LSU ...    38    89   \n","136          US    February 3, 1991   26             Maryland ...    42    82   \n","137          US    November 4, 1991   25       North Carolina ...    45   120   \n","138          US  September 20, 1989   27          Connecticut ...    28    89   \n","139          US      April 26, 1985   32                Texas ...     5    18   \n","140          US  September 23, 1984   32       South Carolina ...    16    70   \n","141          US        May 12, 1988   29          Connecticut ...    56   212   \n","142          US    October 13, 1987   29             Nebraska ...    11    13   \n","\n","     REB  AST  STL  BLK  TO  PTS  DD2  TD3  \n","0     28   12    3    6  12   93    0    0  \n","1    101   72   63   13  40  217    0    0  \n","2     40   78   22    3  24  218    0    0  \n","3    169   65   20   10  38  188    2    0  \n","4     12   12    7    0  14   50    0    0  \n","5     16   11    5    0  11   26    0    0  \n","6     15    5    4    3   3   24    0    0  \n","7     92   95   20   13  59  442    0    0  \n","8    127   40   47   19  37  395    0    0  \n","9     10   10    5    0   2   36    0    0  \n","10   126   50   22    4  32  244    0    0  \n","11   192  136   48   11  87  399    4    0  \n","12    23    7    4    5  12   51    0    0  \n","13    40    5    3    9   6   41    0    0  \n","14     4    1    2    0   3    8    0    0  \n","15    58    8    1   11  16   58    0    0  \n","16    37    9    5    2  10   31    0    0  \n","17    17    3    0    1   3   19    0    0  \n","18     9    2    0    0   7    7    0    0  \n","19   249   78   29   47  68  584    8    0  \n","20    57   58   15    5  44  217    0    0  \n","21    85   52   23    7  31  235    0    0  \n","22    37   98   23    4  53  238    0    0  \n","23    25    2    7    1   7   44    0    0  \n","24     8    5    3    0   2   26    0    0  \n","25   172   39   13   54  52  461    6    0  \n","26   119   59   18   17  49  397    1    0  \n","27   113   42   28   13  50  228    0    0  \n","28   242  127   43   53  80  494   10    1  \n","29   155   47   28   12  42  435    2    0  \n","..   ...  ...  ...  ...  ..  ...  ...  ...  \n","113   52   24   22    0  24  139    0    0  \n","114  180   41   40   17  34  156    0    0  \n","115    2    0    0    0   2    2    0    0  \n","116   28   33   21    3  23   87    0    0  \n","117   82  108   17    1  39  298    1    0  \n","118   24   23   13    5  11   81    0    0  \n","119   17   10   11    1  12   96    0    0  \n","120  122   87   23    8  62  346    1    0  \n","121   38   17   10    1  12   66    0    0  \n","122   86   30   36    2  23  245    0    0  \n","123   48   35   21    4  18  158    0    0  \n","124  107  173   38   24  83  545    1    0  \n","125  156   65   14   37  65  398    3    0  \n","126   86   50   22    8  28  138    0    0  \n","127   53  177   31    3  57  273    1    0  \n","128  106   68   28   17  43  317    0    0  \n","129   14   51   13    2  25   72    0    0  \n","130   21    6    4    3   2   55    0    0  \n","131  297   39   39   61  71  572   16    0  \n","132  110   66   36   14  61  277    0    0  \n","133   34   47   16    1  26  240    0    0  \n","134   16    1    3    3   8   17    0    0  \n","135  127   24   23   22  24  217    1    0  \n","136  124    9   15    7  23  210    0    0  \n","137  165   68   30   16  47  225    2    0  \n","138  117   69   37    8  50  467    0    0  \n","139   23    3    1    3   8   28    0    0  \n","140   86   39   31    5  40  277    0    0  \n","141  268   75   21   22  71  582   11    0  \n","142   24   30   18    1  32  151    0    0  \n","\n","[143 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"7JpGjVQ3JuAo","colab_type":"text"},"cell_type":"markdown","source":["## 1.2 Frequency Distribution Tables"]},{"metadata":{"id":"GiYx25X5J5xx","colab_type":"text"},"cell_type":"markdown","source":["A frequency distribution table has two columns. One column records the unique values of a variable, and the other the frequency of each unique value.\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1eZ42ytzaA8E1VUaV_fUti5-kzOEL4Jc5\">\n","\n","To generate a frequency distribution table using Python, we can use the [Series.value_counts()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html?highlight=value_counts#pandas.Series.value_counts) method. Let's try it on the **Pos** column, which describes the position on the court of each individual.\n","\n","```python\n",">> wnba['Pos'].value_counts()\n","G      60\n","F      33\n","C      25\n","G/F    13\n","F/C    12\n","Name: Pos, dtype: int64\n","```\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","Using the **Series.value_counts()** method, generate frequency distribution tables for the following columns:\n","\n","- **Pos.** Assign the frequency distribution table to a variable named **freq_distro_pos**.\n","- **Height.** Assign the frequency distribution table to a variable named **freq_distro_height**.\n","\n","Using the variable inspector, try to analyze each table and identify how values are distributed and compare to each other.\n","\n"]},{"metadata":{"id":"Dflf1YHtRyQY","colab_type":"code","outputId":"0c592a56-4045-43c6-c6ec-72ebbf839995","executionInfo":{"status":"ok","timestamp":1543337022729,"user_tz":180,"elapsed":805,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":321}},"cell_type":"code","source":["# put your code here\n","data[\"Pos\"].value_counts()\n","data[\"Height\"].value_counts()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["188    20\n","193    18\n","175    16\n","185    15\n","191    11\n","183    11\n","173    11\n","196     9\n","178     8\n","180     7\n","170     6\n","198     5\n","201     2\n","168     2\n","206     1\n","165     1\n","Name: Height, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"Mj8uCsF2Tjsq","colab_type":"text"},"cell_type":"markdown","source":["## 1.3 Sorting Frequency Distribution Tables"]},{"metadata":{"id":"8xTh2fibVAXF","colab_type":"text"},"cell_type":"markdown","source":["As you might have noticed, pandas sorts the tables by default in the descending order of the frequencies. Let's consider again the frequency distribution table for the Pos variable, which is measured on a nominal scale:\n","\n","```python\n",">> wnba['Pos'].value_counts()\n","G      60\n","F      33\n","C      25\n","G/F    13\n","F/C    12\n","Name: Pos, dtype: int64\n","```\n","\n","This default is harmless for variables measured on a nominal scale because the unique values, although different, have no direction (we can't say, for instance, that centers are greater or lower than guards). The default actually helps because we can immediately see which values have the greatest or lowest frequencies, we can make comparisons easily, etc.\n","\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1rF3KN5tNdnTS5Jo8veeBk0Q6ANVSH19x\">\n","\n","\n","For variables measured on **ordinal**, **interval**, or **ratio scales**, this default makes the analysis of the tables more difficult because the unique values have direction (some uniques values are greater or lower than others). Let's consider the table for the **Height** variable, which is measured on a **ratio scale**:\n","\n","```python\n",">> wnba['Height'].value_counts()\n","188    20\n","193    18\n","175    16\n","185    15\n","191    11\n","183    11\n","173    11\n","196     9\n","178     8\n","180     7\n","170     6\n","198     5\n","201     2\n","168     2\n","206     1\n","165     1\n","Name: Height, dtype: int64\n"," ```\n"," \n"," Because the **Height** variable has direction, we might be interested to find:\n","\n","- How many players are under 170 cm?\n","- How many players are very tall (over 185)?\n","- Are there any players below 160 cm?\n","\n","It's time-consuming to answer these questions using the table above. The solution is to sort the table ourselves.\n","\n","**wnba['Height'].value_counts()** returns a **Series** object with the measures of height as indices. This allows us to sort the table by index using the [Series.sort_index()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_index.html) method:\n","\n","```python\n",">> wnba['Height'].value_counts().sort_index()\n","165     1\n","168     2\n","170     6\n","173    11\n","175    16\n","178     8\n","180     7\n","183    11\n","185    15\n","188    20\n","191    11\n","193    18\n","196     9\n","198     5\n","201     2\n","206     1\n","Name: Height, dtype: int64\n","```\n","\n","We can also sort the table by index in a descending order using **wnba['Height'].value_counts().sort_index(ascending = False)**\n","\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","- Generate a frequency distribution table for the **Age** variable, which is measured on a **ratio scale**, and sort the table by unique values.\n","  - Sort the table by unique values in an ascending order, and assign the result to a variable named **age_ascending.**\n","  - Sort the table by unique values in a descending order, and assign the result to a variable named **age_descending.**\n","\n","- Analyze one of the frequency distribution tables and brainstorm questions that might be interesting to answer here. These include:\n","  - How many players are under 20?\n","  - How many players are 30 or over?"]},{"metadata":{"id":"9EqKx2muVyXK","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here\n","data[\"Age\"].value_counts().sort_index(ascending = False)\n","data[\"Age\"].value_counts().sort_index()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yzuk7gHpZMDk","colab_type":"text"},"cell_type":"markdown","source":["## 1.4 Sorting Tables for Ordinal Variables"]},{"metadata":{"id":"xWwd0C87ZbrY","colab_type":"text"},"cell_type":"markdown","source":["The sorting techniques learned in the previous screen can't be used for ordinal scales where the measurement is done using words. We don't have a variable measured on an ordinal scale in our data set, but let's use the **PTS** variable and the conventions below to create one and see why the techniques we learned don't work:\n","\n","\n","<img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=11c0OU4fvss88CyRX7OBAVetRtgpojL2j\">\n","\n","We name the new column **PTS_ordinal_scale**. Below is a short extract from our data set containing the new column:\n","\n","```python\n",">> wnba[['Name', 'PTS', 'PTS_ordinal_scale']].head()\n","```\n","\n","<img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=1TsbZSmvbqIuXkmfXqk_fuknoWSSpFTiL\">\n","\n","\n","Let's examine the frequency distribution table for the **PTS_ordinal_scale** variable:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts()\n","a lot of points    79\n","few points         27\n","many points        25\n","very few points    12\n","dtype: int64\n","```\n","\n","\n","We want to sort the labels in an **ascending** or **descending order**, but using **Series.sort_index()** doesn't work because the method can't infer quantities from words like \"few points\". **Series.sort_index()** can only order the index alphabetically in an ascending or descending order:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts().sort_index()\n","a lot of points    79\n","few points         27\n","many points        25\n","very few points    12\n","dtype: int64\n","```\n","\n","The solution is to do selection by index label. The output of **wnba['PTS_ordinal_scale'].value_counts()** is a Series object with the labels as indices. This means we can select by indices to reorder in any way we like:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts()[['very few points', 'few points', 'many points', 'a lot of points']]\n","very few points    12\n","few points         27\n","many points        25\n","a lot of points    79\n","dtype: int64\n","```\n","\n","This approach can be time-consuming because it involves more typing than it's ideal. We can use **iloc[]** instead to reorder by position:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts().iloc[[3, 1, 2, 0]]\n","very few points    12\n","few points         27\n","many points        25\n","a lot of points    79\n","dtype: int64\n","```\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","We've added more granularity to the ordinal scale above:\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1jYoNYmiO4z0S11WD1Yyj98EnYpZ6ky7S\">\n","\n","\n","Generate a frequency distribution table for the transformed **PTS_ordinal_scale** column.\n","\n","- Order the table by unique values in a descending order (not alphabetically).\n","- Assign the result to a variable named **pts_ordinal_desc.**"]},{"metadata":{"id":"1vPAAgIjb2aI","colab_type":"code","outputId":"057e5d89-36fb-44b6-a328-98610a55fe76","executionInfo":{"status":"ok","timestamp":1543337036423,"user_tz":180,"elapsed":754,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":142}},"cell_type":"code","source":["\n","def make_pts_ordinal(row):\n","    if row['PTS'] <= 20:\n","        return 'very few points'\n","    if (20 < row['PTS'] <=  80):\n","        return 'few points'\n","    if (80 < row['PTS'] <=  150):\n","        return 'many, but below average'\n","    if (150 < row['PTS'] <= 300):\n","        return 'average number of points'\n","    if (300 < row['PTS'] <=  450):\n","        return 'more than average'\n","    else:\n","        return 'much more than average'\n","    \n","wnba['PTS_ordinal_scale'] = wnba.apply(make_pts_ordinal, axis = 1)\n","\n","# put your code here\n","wnba['PTS_ordinal_scale'].value_counts().iloc[[5,1,2,0,3,4]]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["very few points             12\n","few points                  27\n","many, but below average     25\n","average number of points    45\n","more than average           21\n","much more than average      13\n","Name: PTS_ordinal_scale, dtype: int64"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"fuvqRF3jf7Ye","colab_type":"text"},"cell_type":"markdown","source":["## 1.5 Proportions and Percentages"]},{"metadata":{"id":"ThLnlkemgQAF","colab_type":"text"},"cell_type":"markdown","source":["When we analyze distributions, we're often interested in answering questions about **proportions** and **percentages**. For instance, we may want to answer the following questions about the distribution of the **POS** (player position) variable:\n","\n","- What proportion of players are guards?\n","- What percentage of players are centers?\n","- What percentage of players have mixed positions?\n","\n","It's very difficult to answer these questions precisely just by looking at the frequencies:\n","\n","```python\n",">> wnba['Pos'].value_counts()\n","G      60\n","F      33\n","C      25\n","G/F    13\n","F/C    12\n","Name: Pos, dtype: int64\n","```\n","\n","We can see that almost half of the players are guards, but we need more granularity to answer the first question above. For that, we can transform frequencies to proportions.\n","\n","The proportion of each player position quantifies how many players play in a certain position **relative** to the total number of players. There are 60 guards and 143 players overall (including guards) so the proportion of guards is $\\frac{60}{143}$ .\n","\n","In practical data analysis, it's much more common to express the fraction as a decimal between 0 and 1. So we'd say that $0.42$  (the result of $\\frac{60}{143}$ ) of the players are guards.\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1gXcSf6vRp7hBb_SXeZYlB-_PPeZtyFyH\">\n","\n","In pandas, we can compute all the proportions at once by dividing each frequency to the total number of players:\n","\n","```python\n",">> wnba['Pos'].value_counts() / len(wnba)\n","G      0.419580\n","F      0.230769\n","C      0.174825\n","G/F    0.090909\n","F/C    0.083916\n","Name: Pos, dtype: float64\n","```\n","\n","It's slightly faster though to use **Series.value_counts()** with the **normalize** parameter set to **True**:\n","\n","```python\n",">> wnba['Pos'].value_counts(normalize = True)\n","G      0.419580\n","F      0.230769\n","C      0.174825\n","G/F    0.090909\n","F/C    0.083916\n","Name: Pos, dtype: float64\n","```\n","\n","To find percentages, we just have to multiply the proportions by 100:\n","\n","```python\n",">> wnba['Pos'].value_counts(normalize = True) * 100\n","G      41.958042\n","F      23.076923\n","C      17.482517\n","G/F     9.090909\n","F/C     8.391608\n","Name: Pos, dtype: float64\n","```\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1gJzhcwdMVcx2y4DHKHw2LsT_WvyBCz55\">\n","\n","\n","Because proportions and percentages are **relative** to the total number of instances in some set of data, they are called **relative frequencies**. In contrast, the frequencies we've been working with so far are called **absolute frequencies** because they are absolute counts and don't relate to the total number of instances.\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","- Answer the following questions about the Age variable:\n","  - What proportion of players are 25 years old? Assign your answer to a variable named **proportion_25.**\n","  - What percentage of players are 30 years old? Assign your answer to a variable named **percentage_30.**\n","  - What percentage of players are 30 years or older? Assign your answer to a variable named **percentage_over_30**.\n","  - What percentage of players are 23 years or younger? Assign your answer to a variable named **percentage_below_23.**"]},{"metadata":{"id":"K85uB-QXgbUS","colab_type":"code","outputId":"fc81bc90-7c0d-4ff7-a7e3-a2d255406229","executionInfo":{"status":"ok","timestamp":1542913645740,"user_tz":180,"elapsed":756,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":321}},"cell_type":"code","source":["# put your code here\n","wnba['Age'].value_counts(normalize = True) * 100"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24    11.188811\n","25    10.489510\n","23    10.489510\n","28     9.790210\n","27     9.090909\n","26     8.391608\n","22     6.993007\n","30     6.293706\n","32     5.594406\n","31     5.594406\n","29     5.594406\n","34     3.496503\n","35     2.797203\n","33     2.097902\n","21     1.398601\n","36     0.699301\n","Name: Age, dtype: float64"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"si2Ocg5gje7R","colab_type":"text"},"cell_type":"markdown","source":["## 1.6 Percentiles and Percentile Ranks"]},{"metadata":{"id":"sr6UKSMgkDFx","colab_type":"text"},"cell_type":"markdown","source":["In the previous exercise, we found that the percentage of players aged 23 years or younger is 19% (rounded to the nearest integer). This percentage is also called a **percentile rank**.\n","\n","A percentile rank of a value $x$  in a frequency distribution is given by the percentage of values that are equal or less than $x$. In our last exercise, $x=23$, and the fact that 23 has a percentile rank of 19% means that 19% of the values are equal to or less than 23.\n","\n","In this context, the value of 23 is called the **19th percentile**. If a value $x$  is the 19th percentile, it means that 19% of all the values in the distribution are equal to or less than $x$ .\n","\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=17b-Z_fRcvHgwPl3NJP5G-B7Pxy-EZ5c1\">\n","\n","\n","When we're trying to answer questions similar to **\"What percentage of players are 23 years or younger?\"**, we're trying to find **percentile ranks.** In our previous exercise, our answer to this question was 18.881%. We can arrive at the same answer a bit faster using the [percentileofscore(a, score, kind='weak')](https://docs.scipy.org/doc/scipy-0.10.0/reference/generated/scipy.stats.percentileofscore.html#scipy-stats-percentileofscore) function from **scipy.stats**:\n","\n","```python\n",">> from scipy.stats import percentileofscore\n",">> percentileofscore(a = wnba['Age'], score = 23, kind = 'weak')\n","18.88111888111888\n","```\n","\n","We need to use **kind = 'weak'** to indicate that we want to find the percentage of values thar are equal to or less than the value we specify in the score parameter.\n","\n","Another question we had was what percentage of players are 30 years or older. We can answer this question too using percentile ranks. First we need to find the percentage of values equal to or less than 29 years (the percentile rank of 29). The rest of the values must be 30 years or more.\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1tLUg6nvpy5nOhXCWVEJgpyvD7ldhjCVw\">\n","\n","\n","In our exercise the answer we found was 26.573%. This is what we get using the technique we've just learned:\n","\n","```python\n",">> 100 - percentileofscore(wnba['Age'], 29, kind = 'weak')\n","26.573426573426573\n","```\n","\n","In the next sections, we'll learn how to find quickly any percentile using pandas. For now, let's practice percentile ranks more.\n","\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","- Import **percentileofscore()** from scipy.stats, and then use it to answer the following questions:\n","  - What percentage of players played half the number of games or less in the 2016-2017 season (there are 34 games in the WNBA’s regular season)? Use the **Games Played** column to find the data you need, and assign your answer to a variable named **percentile_rank_half_less**.\n","  - What percentage of players played more than half the number of games of the season 2016-2017? Assign your result to **percentage_half_more.**"]},{"metadata":{"id":"lPCCRttbn28k","colab_type":"code","outputId":"7e8439a3-3941-4956-e899-856e7a68bc85","executionInfo":{"status":"ok","timestamp":1543337677183,"user_tz":180,"elapsed":725,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["# put your code here\n","from scipy.stats import percentileofscore\n","percentile_rank_half_less = percentileofscore(data['Games Played'],17, kind = 'weak')\n","percentage_half_more = 100 - percentileofscore(data['Games Played'],17,kind = 'weak')\n","print(percentage_half_more)\n","print(percentile_rank_half_less)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["83.91608391608392\n","16.083916083916083\n"],"name":"stdout"}]},{"metadata":{"id":"BYHbwFmr3au_","colab_type":"text"},"cell_type":"markdown","source":["## 1.7 Finding Percentiles with pandas"]},{"metadata":{"id":"IatkMlRs3nvs","colab_type":"text"},"cell_type":"markdown","source":["To find percentiles, we can use the **Series.describe()** method, which returns by default the **25th**, the **50th**, and the **75th** percentiles:\n","\n","```python\n",">> wnba['Age'].describe()\n","count    143.000000\n","mean      27.076923\n","std        3.679170\n","min       21.000000\n","25%       24.000000\n","50%       27.000000\n","75%       30.000000\n","max       36.000000\n","Name: Age, dtype: float64\n","```\n","\n","We are not interested here in the first three rows of the output (count, mean, and standard deviation). We can use **iloc[]** to isolate just the output we want:\n","\n","```python\n",">>wnba['Age'].describe().iloc[3:]\n","min    21.0\n","25%    24.0\n","50%    27.0\n","75%    30.0\n","max    36.0\n","Name: Age, dtype: float64\n","```\n","\n","The **25th**, **50th**, and **75th** percentiles pandas returns by default are the scores that divide the distribution into four equal parts.\n","\n","<img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1UI-X7sTvPKAa4xkygiBM8cxkxjpeCBlL\">\n","\n","\n","The three percentiles that divide the distribution in four equal parts are also known as **quartiles** (from the Latin [quartus](http://www.latin-dictionary.net/definition/32600/quattuor-quartus) which means four). There are three quartiles in the distribution of the **Age** variable:\n","\n","- The first quartile (also called lower quartile) is 24 (note that 24 is also the 25th percentile).\n","- The second quartile (also called the middle quartile) is 27 (note that 27 is also the 50th percentile).\n","- And the third quartile (also called the upper quartile) is 30 (note that 30 is also the 75th percentile).\n","\n","We may be interested to find the percentiles for percentages other than 25%, 50%, or 75%. For that, we can use the **percentiles** parameter of **Series.describe()**. This parameter requires us to pass the **percentages** we want as proportions between 0 and 1.\n","\n","\n","```python\n",">> wnba['Age'].describe(percentiles = [.1, .15, .33, .5, .592, .85, .9]).iloc[3:]\n","min      21.0\n","10%      23.0\n","15%      23.0\n","33%      25.0\n","50%      27.0\n","59.2%    28.0\n","85%      31.0\n","90%      32.0\n","max      36.0\n","Name: Age, dtype: float64\n","```\n","\n","Percentiles don't have a single [standard definition](https://en.wikipedia.org/wiki/Percentile#Definitions), so don't be surprised if you get very similar (but not identical) values if you use different functions (especially if the functions come from different libraries).\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","- Use the **Age** variable along with **Series.describe()** to answer the following questions:\n","  - What's the upper quartile of the **Age** variable? Assign your answer to a variable named **age_upper_quartile.**\n","  - What's the middle quartile of the **Age** variable? Assign your answer to a variable named **age_middle_quartile.**\n","  - What's the **95th percentile** of the **Age** variable? Assign your answer to a variable named **age_95th_percentile.**\n","- Indicate the truth value of the following sentences:\n","  - A **percentile** is a value of a variable, and it corresponds to a certain **percentile rank** in the distribution of that variable. (If you think this is true, assign **True** (boolean, not string) to a variable named **question_1**, otherwise assign **False**.)\n","  - A **percentile rank** is a numerical value from the distribution of a variable. (Assign **True** or **False** to **question_2**.)\n","  - The **25th percentile** is the same thing as the **lower quartile**, and the **upper quartile** is the same thing as the **third quartile**. (Assign **True** or **False** to **question_3**)\n","\n"]},{"metadata":{"id":"ioZotk1Q3wPn","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here\n","age_upper_quartile = data['Age'].describe(percentiles = [.75]).iloc[5]\n","age_middle_quartile = data['Age'].describe(percentiles = [.50]).iloc[4]\n","age_95th_percentile = data['Age'].describe(percentiles = [.95]).iloc[5]\n","\n","question_1 = False\n","question_2 = True\n","question_3 = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W5ai_b-zBbWM","colab_type":"text"},"cell_type":"markdown","source":["## 2.8 Grouped Frequency Distribution Tables"]},{"metadata":{"id":"U4ctTbziBowH","colab_type":"text"},"cell_type":"markdown","source":["With frequency tables, we're trying to transform relatively large and incomprehensible amounts of data to a table format we can understand. However, not all frequency tables are straightforward:\n","\n","\n","```python\n",">> wnba['Weight'].value_counts().sort_index()\n","55.0      1\n","57.0      1\n","58.0      1\n","59.0      2\n","62.0      1\n","63.0      3\n","64.0      5\n","65.0      4\n","66.0      8\n","67.0      1\n","68.0      2\n","69.0      2\n","70.0      3\n","71.0      2\n","73.0      6\n","74.0      4\n","75.0      4\n","76.0      4\n","77.0     10\n","78.0      5\n","79.0      6\n","80.0      3\n","81.0      5\n","82.0      4\n","83.0      4\n","84.0      9\n","85.0      2\n","86.0      7\n","87.0      6\n","88.0      6\n","89.0      3\n","90.0      2\n","91.0      3\n","93.0      3\n","95.0      2\n","96.0      2\n","97.0      1\n","104.0     2\n","108.0     1\n","113.0     2\n","Name: Weight, dtype: int64\n","```\n","\n","\n","There's a lot of granularity in the table above, but for this reason it's not easy to find patterns. The table for the **Weight** variable is a relatively happy case - the frequency tables for variables like **PTS**, **BMI**, or **MIN** are even more **daunting**.\n","\n","If the variable is measured on an **interval** or **ratio scale**, a common solution to this problem is to **group the values in equal intervals**. For the **Weight** variable, the values range from 55 to 113 kg, which amounts to a difference of 58 kg. We can try to segment this 58 kg interval in ten smaller and equal intervals. This will result in ten intervals of 5.8 kg each:\n","\n","<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=12ar9ft_-oVZpZWH_hcjz3awBmhB-ADHg\">\n","\n","\n","Fortunately, pandas can handle this process gracefully. We only need to make use of the **bins** parameter of **Series.value_counts()**. We want ten equal intervals, so we need to specify **bins = 10**:\n","\n","\n","```python\n",">> wnba['Weight'].value_counts(bins = 10).sort_index()\n","(54.941, 60.8]     5\n","(60.8, 66.6]      21\n","(66.6, 72.4]      10\n","(72.4, 78.2]      33\n","(78.2, 84.0]      31\n","(84.0, 89.8]      24\n","(89.8, 95.6]      10\n","(95.6, 101.4]      3\n","(101.4, 107.2]     2\n","(107.2, 113.0]     3\n","Name: Weight, dtype: int64\n"," ```\n"," \n","(54.941, 60.8], (60.8, 66.6] or (107.2, 113.0] are number intervals. The ( character indicates that the starting point is not included, while the ] indicates that the endpoint is included. (54.941, 60.8] means that 54.941 isn't included in the interval, while 60.8 is. The interval (54.941, 60.8] contains all real numbers greater than 54.941 and less than or equal to 60.8.\n","\n","We can see above that there are 10 equal intervals, 5.8 each. The first interval, (54.941, 60.8] is confusing, and has to do with [how pandas internals show the output](https://github.com/pandas-dev/pandas/blob/01e99decf14b55409cea0789ffcc615afed45bac/pandas/core/algorithms.py#L497). One way to understand this is to convert 54.941 to 1 decimal point, like all the other values are. Then the first interval becomes (54.9, 60.8]. 54.9 is not included, so you can think that the interval starts at the minimum value of the Weight variable, which is 55.\n","\n","Because we group values in a table to get a better sense of frequencies in the distribution, the table we generated above is also known as a **grouped frequency distribution table**. Each group (interval) in a grouped frequency distribution table is also known as a **class interval**. (107.2, 113.0], for instance, is a class interval.\n","\n","Using the grouped frequency distribution table we generated above for the **Weight** variable, we can find patterns easier in the distribution of values:\n","\n","- Most players weigh somewhere between 70 and 90 kg.\n","- Very few players weigh over 100 kg.\n","- Very few players weigh under 60 kg; etc.\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","Verify on the cell below and examine the frequency table for the **PTS** (total points) variable trying to find some patterns in the distribution of values. Then, generate a grouped frequency distribution table for the **PTS** variable with the following characteristics:\n","- The table has 10 class intervals.\n","- For each class interval, the table shows percentages instead of frequencies.\n","- The class intervals are sorted in descending order.\n","- Assign the table to a variable named **grouped_freq_table**, then print it and try again to find some patterns in the distribution of values.\n"]},{"metadata":{"id":"DGJwq4tdBz3H","colab_type":"code","outputId":"7d6fff3d-c6c7-43f1-9c33-a1c743c32371","executionInfo":{"status":"ok","timestamp":1543338619298,"user_tz":180,"elapsed":766,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":214}},"cell_type":"code","source":["# put your code here\n","grouped_freq_table = data['PTS'].value_counts(bins = 10,normalize = True).sort_index()\n","print(grouped_freq_table)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1.417, 60.2]     0.209790\n","(60.2, 118.4]     0.167832\n","(118.4, 176.6]    0.118881\n","(176.6, 234.8]    0.139860\n","(234.8, 293.0]    0.118881\n","(293.0, 351.2]    0.055944\n","(351.2, 409.4]    0.069930\n","(409.4, 467.6]    0.055944\n","(467.6, 525.8]    0.027972\n","(525.8, 584.0]    0.034965\n","Name: PTS, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"tNSbzczYEI-A","colab_type":"text"},"cell_type":"markdown","source":["## 1.9 Information Loss"]},{"metadata":{"id":"9_wnh6DxEP9g","colab_type":"text"},"cell_type":"markdown","source":["When we generate grouped frequency distribution tables, there's an inevitable information loss. Let's consider this table:\n","\n","```python\n",">> wnba['PTS'].value_counts(bins = 10)\n","(1.417, 60.2]     30\n","(60.2, 118.4]     24\n","(118.4, 176.6]    17\n","(176.6, 234.8]    20\n","(234.8, 293.0]    17\n","(293.0, 351.2]     8\n","(351.2, 409.4]    10\n","(409.4, 467.6]     8\n","(467.6, 525.8]     4\n","(525.8, 584.0]     5\n","Name: PTS, dtype: int64\n"," ```\n"," \n","Looking at the first interval, we can see there are 30 players who scored between 2 and 60 points (2 is the minimum value in our data set, and points in basketball can only be integers). However, because we grouped the values, we lost more granular information like:\n","\n","- How many players, if any, scored exactly 50 points.\n","- How many players scored under 10 points.\n","- How many players scored between 20 and 30 points, etc.\n","\n","To get back this granular information, we can increase the number of class intervals. However, if we do that, we end up again with a table that's lengthy and very difficult to analyze.\n","\n","On the other side, if we decrease the number of class intervals, we lose even more information:\n","\n","```python\n","wnba['PTS'].value_counts(bins = 5).sort_index()\n","(1.417, 118.4]    54\n","(118.4, 234.8]    37\n","(234.8, 351.2]    25\n","(351.2, 467.6]    18\n","(467.6, 584.0]     9\n","Name: PTS, dtype: int64\n","```\n","\n","There are 54 players that scored between 2 and 118 points. We can get this information from the first table above too, but there's some extra information there: among these 54 players, 30 scored between 2 and 60 points, and 24 scored between 61 and 118 points. We lost this information when we decreased the number of class intervals from 10 to 5.\n","\n","We can conclude there is a trade-off between the information in a table, and how comprehensible the table is.\n","\n","\n","<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=1PtWCiK-ocbotjHqhFxmWL-3j_kAdP9au\">\n","\n","\n","\n","When we increase the number of class intervals, we can get more information, but the table becomes harder to analyze. When we decrease the number of class intervals, we get a boost in comprehensibility, but the amount of information in the table decreases.\n","\n","As a rule of thumb, 10 is a good number of class intervals to choose because it offers a good balance between information and comprehensibility.\n","\n","<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=1MaCcxrDyJhXd6smlAt-eK41rqRImqKyi\">\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","Generate a grouped frequency distribution for the **MIN** variable (minutes played during the season), and experiment with the number of class intervals to get a sense for what conclusions you can draw as you vary the number of class intervals. Try to experiment with the following numbers of class intervals:\n","- 1\n","- 2\n","- 3\n","- 5\n","- 10\n","- 15\n","- 20\n","- 40"]},{"metadata":{"id":"aalHcCmoGYuP","colab_type":"code","outputId":"4d7e1c42-5f90-4f60-c693-8b27e0859a7c","executionInfo":{"status":"ok","timestamp":1543338904782,"user_tz":180,"elapsed":701,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":750}},"cell_type":"code","source":["# put your code here\n","data['MIN'].value_counts(bins=40).sort_index()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10.993, 37.15]     5\n","(37.15, 62.3]       7\n","(62.3, 87.45]       3\n","(87.45, 112.6]      4\n","(112.6, 137.75]     4\n","(137.75, 162.9]     2\n","(162.9, 188.05]     1\n","(188.05, 213.2]     3\n","(213.2, 238.35]     7\n","(238.35, 263.5]     4\n","(263.5, 288.65]     4\n","(288.65, 313.8]     2\n","(313.8, 338.95]     2\n","(338.95, 364.1]     4\n","(364.1, 389.25]     6\n","(389.25, 414.4]     3\n","(414.4, 439.55]     1\n","(439.55, 464.7]     5\n","(464.7, 489.85]     3\n","(489.85, 515.0]     4\n","(515.0, 540.15]     3\n","(540.15, 565.3]     1\n","(565.3, 590.45]     3\n","(590.45, 615.6]     5\n","(615.6, 640.75]     4\n","(640.75, 665.9]     3\n","(665.9, 691.05]     5\n","(691.05, 716.2]     3\n","(716.2, 741.35]     5\n","(741.35, 766.5]     5\n","(766.5, 791.65]     1\n","(791.65, 816.8]     3\n","(816.8, 841.95]     6\n","(841.95, 867.1]     6\n","(867.1, 892.25]     3\n","(892.25, 917.4]     4\n","(917.4, 942.55]     3\n","(942.55, 967.7]     4\n","(967.7, 992.85]     0\n","(992.85, 1018.0]    2\n","Name: MIN, dtype: int64"]},"metadata":{"tags":[]},"execution_count":55}]},{"metadata":{"id":"pUzyuKd3EUAC","colab_type":"text"},"cell_type":"markdown","source":["## 1.10 Readability for Grouped Frequency Tables"]},{"metadata":{"id":"QgCqGq7EGN7L","colab_type":"text"},"cell_type":"markdown","source":["Pandas helps a lot when we need to explore quickly grouped frequency tables. However, the intervals pandas outputs are confusing at a first sight:\n","\n","```python\n","wnba['PTS'].value_counts(bins = 5).sort_index()\n","(1.417, 118.4]    54\n","(118.4, 234.8]    37\n","(234.8, 351.2]    25\n","(351.2, 467.6]    18\n","(467.6, 584.0]     9\n","Name: PTS, dtype: int64\n","```\n","\n","Imagine we'd have to publish the table above in a blog post or a scientific paper. The readers will have a hard time understanding the intervals we chose. They'll also be puzzled by the decimal numbers because points in basketball can only be integers.\n","\n","To fix this, we can define the intervals ourselves. For the table above, we can define six intervals of 100 points each, and then count how many values fit in each interval. We'd like to end with a table like this:\n","\n","\n","```python\n","(0,100]      49\n","(100,200]    28\n","(200,300]    32\n","(300,400]    17\n","(400,500]    10\n","(500,600]     7\n","```\n","\n","Next, we show one way to code the intervals. We start with creating the intervals using the [pd.interval_range()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.interval_range.html?highlight=interval_rang) function:\n","\n","```python\n",">> intervals = pd.interval_range(start = 0, end = 600, freq = 100)\n",">> intervals\n","IntervalIndex([(0, 100], (100, 200], (200, 300], (300, 400], (400, 500], (500, 600]]\n","              closed='right',\n","              dtype='interval[int64]')\n","```\n","\n","              \n","Next, we create a new Series using the intervals as indices, and, for now, 0 as values:\n","\n","```python\n",">> gr_freq_table = pd.Series([0,0,0,0,0,0], index = intervals)\n",">> gr_freq_table\n","(0, 100]      0\n","(100, 200]    0\n","(200, 300]    0\n","(300, 400]    0\n","(400, 500]    0\n","(500, 600]    0\n","dtype: int64\n","```\n","\n","Next, we loop through the values of the **PTS** column, and for each value:\n","\n","- We loop through the intervals we defined previously, and for each interval:\n","  - We check whether the current value from the PTS column belongs to that interval.\n","  - If the value doesn't belong to an interval, we continue the inner loop over the intervals.\n","  - If the value belongs to an interval:\n","    - We update the counting for that interval in **gr_freq_table** by adding 1.\n","    - We exit the inner loop over the intervals with **break** because a value can belong to one interval only, and it makes no sense to continue the loop (without using **break**, we'll get the same output but we'll do many redundant iterations).\n","    \n","    \n","```python\n",">> for value in wnba['PTS']:\n","       for interval in intervals:\n","           if value in interval:\n","               gr_freq_table.loc[interval] += 1\n","               break\n",">> gr_freq_table\n","(0, 100]      49\n","(100, 200]    28\n","(200, 300]    32\n","(300, 400]    17\n","(400, 500]    10\n","(500, 600]     7\n","dtype: int64\n","```\n","\n","\n","Now we do a quick sanity check of our work. There are 143 players in the data set, so the frequencies should add up to 143:\n","\n","```python\n",">> gr_freq_table.sum()\n","143\n","```\n","\n","Note that we're not restricted by the minimum and maximum values of a variable when we define intervals. The minimum number of points is 2, and the maximum is 584, but our intervals range from 1 to 600.\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","- Using the techniques above, generate a grouped frequency table for the **PTS** variable. The table should have the following characteristics:\n","  - There are 10 class intervals.\n","  - The first class interval starts at 0 (not included).\n","  - The last class interval ends at 600 (included).\n","  - Each interval has a range of 60 points.\n","- Assign the table to a variable named **gr_freq_table_10**."]},{"metadata":{"id":"aNCIWc8gG0eo","colab_type":"code","outputId":"6fcb3dc3-c302-41ea-e5b0-0183e3b29dc3","executionInfo":{"status":"ok","timestamp":1543339189574,"user_tz":180,"elapsed":707,"user":{"displayName":"Carlos Vinícius Santos","photoUrl":"https://lh5.googleusercontent.com/-3f41s0dwR6s/AAAAAAAAAAI/AAAAAAAAAGs/RJNYITniAXI/s64/photo.jpg","userId":"06131293188032133705"}},"colab":{"base_uri":"https://localhost:8080/","height":214}},"cell_type":"code","source":["# put your code here\n","intervals = pd.interval_range(start = 0, end = 600, freq = 60)\n","gr_freq_table = pd.Series([0,0,0,0,0,0,0,0,0,0], index = intervals)\n","\n","for value in wnba['PTS']:\n","       for interval in intervals:\n","           if value in interval:\n","               gr_freq_table.loc[interval] += 1\n","               break\n","gr_freq_table             "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 60]       30\n","(60, 120]     25\n","(120, 180]    17\n","(180, 240]    22\n","(240, 300]    15\n","(300, 360]     7\n","(360, 420]    11\n","(420, 480]     7\n","(480, 540]     4\n","(540, 600]     5\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":63}]},{"metadata":{"id":"uo0Qb2l_IZ75","colab_type":"text"},"cell_type":"markdown","source":["## 1.11 Frequency Tables and Continuous Variables\n"]},{"metadata":{"id":"DOLArazU4k1H","colab_type":"text"},"cell_type":"markdown","source":["Remember from the previous mission that a height of 175 cm is just an interval bounded by the real limits of 174.5 cm (lower real limit) and 175.5 (upper real limit). When we build frequency tables for continuous variables, we need to take into account that the values are intervals.\n","\n","The height of 175 cm has a frequency of 16 in the distribution of the **Height** variable:\n","\n","```python\n",">> wnba['Height'].value_counts()[175]\n","16\n","```\n","\n","This doesn't mean that there are 16 players that are all exactly 175 cm tall. It rather means that there are 16 players with a height that's somewhere between 174.5 cm and 175.5 cm.\n","\n","A similar reasoning applies when we read grouped frequency tables. If we had an interval of (180, 190] for a continuous variable, 180 and 190 are not the real limits. Instead, the real limits are given by the interval (179.5, 190.5], with 179.5 being the lower real limit of 180, and 190.5 the upper real limit of 190.\n","\n","Continuous variables affect as well the way we read percentiles. For instance, the 50th percentile (middle quartile) in the distribution of the **Height** variable is 185 cm:\n","\n","\n","```python\n",">> wnba['Height'].describe().iloc[3:]\n","min    165.0\n","25%    176.5\n","50%    185.0\n","75%    191.0\n","max    206.0\n","Name: Height, dtype: float64\n","```\n","\n","This means that 50% of the values are less or equal to 185.5 cm (the upper limit of 185 cm), not to 185 cm.\n","\n","\n"]},{"metadata":{"id":"fQaUp50e47Hr","colab_type":"text"},"cell_type":"markdown","source":["## 1.12 Next Steps\n","\n"]},{"metadata":{"id":"SlMNgBBT55da","colab_type":"text"},"cell_type":"markdown","source":["In this mission, we learned how to organize data in frequency and grouped frequency tables. Frequency tables allow us to transform large and incomprehensible amounts of data to a format we can understand.\n","\n","\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1Zp0BuFcG3T0rk4MSaPfw8gpGPdlrQ68I\">\n","\n","Next in the course, we'll learn how to visualize frequency tables using bar plots and histograms.\n","\n"]}]}